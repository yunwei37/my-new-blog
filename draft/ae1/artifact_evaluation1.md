# Artifact Evaluation for "FlashOverlap: Efficient Communication-Computation Overlap for Multi-GPU Computing"

## Paper Summary
Summarize the paper.

This paper presents FlashOverlap, a tile-wise overlapping design for multi-GPU systems that addresses three requirements: tile-wise overlapping, interference-free computation, and communication agnosticism. The design uses a novel signaling mechanism where computation kernels send signals when output portions finish, triggering communication while continuing computation. Key components include signaling timing optimization and pre/post-communication reordering for contiguous data addresses. Experiments show up to 1.65× speedup across communication primitives (all_reduce, all_to_all, reduce_scatter) and multi-GPU configurations.

## Artifact Summary
Summarize the artifacts.

The artifact is available at https://anonymous.4open.science/r/overlap-code-8F50/README.md. The submitted artifacts include the overlap-code implementation with signal mechanism for computation-communication overlap using two CUDA streams. The artifact provides CUTLASS GEMM wrappers, signal-based overlap implementation, and communication reordering components. It includes configuration generation tools, optimization framework with exhaustive and predictive search methods, and evaluation suite with correctness and performance testing scripts. The implementation supports GEMM shapes (M, N % 128 == 0) and includes `ReorderRMSNorm` class and `OverlapRowParallelLayer` class that can replace standard `RMSNorm` and `RowParallelLayer` classes for end-to-end inference or training. Dependencies include NCCL (v2.18.3/v2.19.3), CUTLASS (v3.6.0/v3.9.0), PyTorch 2.5.1, and CUDA 12.1/12.2, the artifact stated it was tested on RTX 3090, RTX 4090, A800, and A100 GPUs.

## Environment Used for Evaluation
Describe the environment.

The evaluation was conducted using a server equipped with 8 RTX 4090 GPUs, as the authors provided.

## Steps Used for Evaluation
Describe the evaluation steps.

The evaluation followed the README.md build instructions: cloning the repository, initializing CUTLASS submodule with `git submodule update --init --recursive`, installing dependencies, generating GEMM instances using `python generate_instances.py`, and building with CMake (>=3.18). Configuration generation used the custom profiler with `python profile_config.py`. Tuning involved exhaustive search using `python search.py` on the 8-GPU server. Correctness testing executed GEMM+AllReduce+RMSNorm combinations using `python correctness.py`, and performance evaluation ran speedup tests with `python test.py`.

## Support for the Paper's Claims
Do the artifacts support the paper's claims?

The artifacts support the paper's claims with some minor discrepancies. The signal-based overlap mechanism works correctly with communication primitives properly integrated. The GEMM+communication+RMSNorm pipeline functions as described, with correctness tests passing for all_reduce operations. Performance results show speedups across all primitives: all_reduce (1.12×-1.31×), all_to_all (1.17×-1.35×), and reduce_scatter (1.15×-1.32×). The predictive search achieves 99.02% of exhaustive search performance, and overhead analysis shows minimal impact (0.03% tile reorder, 0.35% token reorder). However, there are some differences from paper-reported numbers: predictive search shows 9.0158 ± 4.7713% error ratio. The authors attribute these differences to test data changes including random sampling for evaluation efficiency and addition of small-sized cases to address reviewer feedback. Some reduce_scatter tests failed due to PyTorch compatibility issues with `torch.unravel_index`, but all_reduce tests passed successfully.

Speedup Statistics by Primitive and GPU Combination:
===============================================================================================
Primitive       GPU        Baseline             Decomposition        FlashOverlap        
-----------------------------------------------------------------------------------------------
all_reduce      2          1.00(1.00-1.00)      1.20(1.16-1.25)      1.31(1.18-1.54)     
all_reduce      4          1.00(1.00-1.00)      1.17(1.11-1.23)      1.22(1.13-1.40)     
all_reduce      8          1.00(1.00-1.00)      1.13(1.07-1.18)      1.12(1.05-1.26)     
all_to_all      2          1.00(1.00-1.00)      1.24(1.14-1.38)      1.35(1.01-1.70)     
all_to_all      4          1.00(1.00-1.00)      1.11(1.06-1.22)      1.26(1.15-1.50)     
all_to_all      8          1.00(1.00-1.00)      0.98(0.91-1.05)      1.17(1.06-1.25)     
reduce_scatter  2          1.00(1.00-1.00)      1.19(1.11-1.24)      1.32(1.24-1.40)     
reduce_scatter  4          1.00(1.00-1.00)      1.19(1.15-1.24)      1.30(1.13-1.50)     
reduce_scatter  8          1.00(1.00-1.00)      1.19(1.11-1.30)      1.15(1.06-1.34)     
===============================================================================================
Note: Speedup values shown as mean(min-max), where baseline = 1.00

## Comments for Authors
Explain ratings and suggest improvements.

Thanks for the artifacts! You can add performance tuning guidelines for different hardware configurations, and include estimated time and progress indicators for long-running experiments like parameter tuning.

## Comments for AEC (hidden from authors)
Comments for AEC reviewers.

This artifact demonstrates exceptional quality and completeness, representing a sophisticated implementation that addresses a real performance bottleneck in multi-GPU systems. The evaluation results strongly support the paper's claims with consistent speedup improvements across different configurations. The exhaustive search methodology, while computationally intensive, provides thorough optimization that validates the predictive approach, and the modular design makes the code maintainable and extensible for future research. The only significant issue is the PyTorch compatibility problem affecting some reduce_scatter tests, but this doesn't undermine the core contribution since all_reduce tests pass completely and the performance results align with expectations. This artifact fully deserves all three badges and represents a high-quality contribution to the systems research community, making it a strong candidate for the Distinguished Artifact Award.

---

**Reviewer Expertise**: Expert (4/4) - Extensive experience with GPU computing, communication primitives, and performance optimization.

**Badges Awarded**: 
- ✅ Artifact Available: Accept (4)
- ✅ Artifact Functional: Accept (4)  
- ✅ Results Reproduced: Accept (4)

**Distinguished Artifact Award**: Yes (2)

[[252, 252, 252, 268], [128, 0, 0, 0], [252, 252, 8, 0], [252, 132, 0, 0], [128, 0, 0, 0], [252, 132, 0, 0], [128, 0, 0, 0], [252, 252, 252, 652]] 45.4936
[[252, 252, 504, 16], [128, 0, 0, 0], [252, 252, 8, 0], [252, 132, 0, 0], [128, 0, 0, 0], [252, 132, 0, 0], [128, 0, 0, 0], [252, 252, 504, 400]] 51.9576

[[252, 252, 520], [128, 0, 0], [252, 252, 8], [252, 132, 0], [128, 0, 0], [252, 132, 0], [128, 0, 0], [252, 252, 904]] 42.0003
[[252, 504, 268], [128, 0, 0], [252, 260, 0], [252, 132, 0], [128, 0, 0], [252, 132, 0], [128, 0, 0], [252, 504, 652]] 51.4569
[[252, 772], [128, 0], [252, 260], [252, 132], [128, 0], [252, 132], [128, 0], [252, 1156]] 47.2156
[[504, 504, 16], [128, 0, 0], [504, 8, 0], [384, 0, 0], [128, 0, 0], [384, 0, 0], [128, 0, 0], [504, 504, 400]] 58.0124
[[504, 520], [128, 0], [504, 8], [384, 0], [128, 0], [384, 0], [128, 0], [504, 904]] 49.6674
[[756, 268], [128, 0], [512, 0], [384, 0], [128, 0], [384, 0], [128, 0], [756, 652]] 57.4156
[[1024], [128], [512], [384], [128], [384], [128], [1408]] 50.6750
Best solution:  [[252, 252, 520], [128, 0, 0], [252, 252, 8], [252, 132, 0], [128, 0, 0], [252, 132, 0], [128, 0, 0], [252, 252, 904]]
Solution saved.
 50%|██████████████████████████████▌                              | 4/8 [33:15<43:20, 650.14s/it]Transfer matrix: 
tensor([[3584, 3072],
        [ 512, 1024]], dtype=torch.int32)
Start exhaustive searching.
[[252, 252, 252, 252, 252, 252, 152], [252, 132, 0, 0, 0, 0, 0]] 3.9437
[[252, 252, 252, 252, 252, 404], [252, 132, 0, 0, 0, 0]] 4.2206
[[252, 252, 252, 252, 656], [252, 132, 0, 0, 0]] 4.6896
[[252, 252, 252, 504, 404], [252, 132, 0, 0, 0]] 4.2318
[[252, 252, 252, 908], [252, 132, 0, 0]] 5.2236
[[252, 252, 504, 656], [252, 132, 0, 0]] 4.7125
[[252, 252, 1160], [252, 132, 0]] 5.7623
[[252, 504, 504, 404], [252, 132, 0, 0]] 4.2184
[[252, 504, 908], [252, 132, 0]] 5.2480
[[252, 756, 656], [252, 132, 0]] 4.7130
[[252, 1412], [252, 132]] 6.2610
[[504, 504, 656], [384, 0, 0]] 4.6759
[[504, 1160], [384, 0]] 5.8485
[[756, 908], [384, 0]] 5.2264
[[1664], [384]] 6.6362
Best solution:  [[252, 252, 252, 252, 252, 252, 152], [252, 132, 0, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[   0,    0,  512,  512],
        [3072, 1024, 1024, 1536],
        [1024, 3072, 2560, 1536],
        [   0,    0,    0,  512]], dtype=torch.int32)
Start exhaustive searching.
[[252, 4, 0, 0, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152, 0, 0], [252, 252, 252, 252, 252, 252, 252, 252, 32], [128, 0, 0, 0, 0, 0, 0, 0, 0]] 14.9489
[[252, 4, 0, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152, 0], [252, 252, 252, 252, 252, 252, 252, 284], [128, 0, 0, 0, 0, 0, 0, 0]] 15.1642
[[252, 4, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152], [252, 252, 252, 252, 252, 252, 536], [128, 0, 0, 0, 0, 0, 0]] 15.9271
[[252, 4, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 404, 0], [252, 252, 252, 252, 252, 504, 284], [128, 0, 0, 0, 0, 0, 0]] 14.8686
[[252, 4, 0, 0, 0, 0], [252, 252, 252, 252, 252, 404], [252, 252, 252, 252, 252, 788], [128, 0, 0, 0, 0, 0]] 15.3858
[[252, 4, 0, 0, 0, 0], [252, 252, 252, 252, 504, 152], [252, 252, 252, 252, 504, 536], [128, 0, 0, 0, 0, 0]] 16.0170
[[252, 4, 0, 0, 0], [252, 252, 252, 252, 656], [252, 252, 252, 252, 1040], [128, 0, 0, 0, 0]] 14.9368
[[252, 4, 0, 0, 0, 0], [252, 252, 252, 504, 404, 0], [252, 252, 252, 504, 504, 284], [128, 0, 0, 0, 0, 0]] 14.5623
[[252, 4, 0, 0, 0], [252, 252, 252, 504, 404], [252, 252, 252, 504, 788], [128, 0, 0, 0, 0]] 15.0842
[[252, 4, 0, 0, 0], [252, 252, 252, 756, 152], [252, 252, 252, 756, 536], [128, 0, 0, 0, 0]] 15.5075
[[252, 4, 0, 0], [252, 252, 252, 908], [252, 252, 252, 1292], [128, 0, 0, 0]] 13.6697
[[252, 4, 0, 0, 0], [252, 252, 504, 504, 152], [252, 252, 504, 504, 536], [128, 0, 0, 0, 0]] 15.7292
[[252, 4, 0, 0], [252, 252, 504, 656], [252, 252, 504, 1040], [128, 0, 0, 0]] 14.5490
[[252, 4, 0, 0], [252, 252, 756, 404], [252, 252, 756, 788], [128, 0, 0, 0]] 15.4040
[[252, 4, 0], [252, 252, 1160], [252, 252, 1544], [128, 0, 0]] 14.4129
[[252, 4, 0, 0, 0], [252, 504, 504, 404, 0], [252, 504, 504, 504, 284], [128, 0, 0, 0, 0]] 14.1840
[[252, 4, 0, 0], [252, 504, 504, 404], [252, 504, 504, 788], [128, 0, 0, 0]] 14.6955
[[252, 4, 0, 0], [252, 504, 756, 152], [252, 504, 756, 536], [128, 0, 0, 0]] 15.2321

[[252, 4, 0], [252, 504, 908], [252, 504, 1292], [128, 0, 0]] 13.5863
[[252, 4, 0], [252, 756, 656], [252, 756, 1040], [128, 0, 0]] 14.3782
[[252, 4, 0], [252, 1008, 404], [252, 1008, 788], [128, 0, 0]] 15.1689
[[252, 4], [252, 1412], [252, 1796], [128, 0]] 14.4152
[[256, 0, 0, 0], [504, 504, 504, 152], [504, 504, 504, 536], [128, 0, 0, 0]] 15.8649
[[256, 0, 0], [504, 504, 656], [504, 504, 1040], [128, 0, 0]] 14.8846
[[256, 0, 0], [504, 756, 404], [504, 756, 788], [128, 0, 0]] 15.3008
[[256, 0], [504, 1160], [504, 1544], [128, 0]] 15.5038

[[256, 0, 0], [756, 756, 152], [756, 756, 536], [128, 0, 0]] 16.1964
[[256, 0], [756, 908], [756, 1292], [128, 0]] 14.1633
[[256, 0], [1008, 656], [1008, 1040], [128, 0]] 15.5432

[[256], [1664], [2048], [128]] 15.7892
Best solution:  [[252, 4, 0], [252, 504, 908], [252, 504, 1292], [128, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[ 512,    0,    0, 1024,  512, 1024,  512, 1536],
        [   0,    0,    0,    0,  512,    0,    0,    0],
        [1024, 1536,    0,  512, 1536,  512,    0, 1536],
        [   0,  512,  512,  512,  512, 1024, 2048,  512],
        [   0,  512,    0,    0,    0,    0,    0,    0],
        [   0, 1024, 1024,  512,    0,    0,  512,    0],
        [   0,    0,  512,    0,    0,    0,    0,    0],
        [2560,  512, 2048, 1536, 1024, 1536, 1024,  512]], dtype=torch.int32)
Start exhaustive searching.
[[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0, 0], [378, 378, 378, 274, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 42]] 88.3877
[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0], [378, 378, 378, 274, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 420]] 87.9709
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0], [378, 378, 378, 274, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 798]] 88.7878
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0], [378, 378, 378, 274, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 420]] 89.3156
[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 152], [378, 378, 378, 274, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1176]] 89.4381
[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 530, 0], [378, 378, 378, 274, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 798]] 93.7558
[[378, 378, 378, 146], [128, 0, 0, 0], [378, 378, 378, 530], [378, 378, 378, 274], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 378, 1554]] 93.1054
[[378, 378, 524, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 152, 0], [378, 378, 652, 0, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 420]] 91.2235
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 756, 152], [378, 378, 652, 0], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 756, 1176]] 93.1747
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 908, 0], [378, 378, 652, 0], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 1134, 798]] 90.4012

[[378, 378, 524], [128, 0, 0], [378, 378, 908], [378, 378, 652], [128, 0, 0], [378, 378, 12], [128, 0, 0], [378, 378, 1932]] 90.7701
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 530, 0], [378, 756, 274, 0], [128, 0, 0, 0], [378, 390, 0, 0], [128, 0, 0, 0], [378, 756, 756, 798]] 96.5554
[[378, 756, 146], [128, 0, 0], [378, 756, 530], [378, 756, 274], [128, 0, 0], [378, 390, 0], [128, 0, 0], [378, 756, 1554]] 94.8077
[[378, 902, 0], [128, 0, 0], [378, 1134, 152], [378, 1030, 0], [128, 0, 0], [378, 390, 0], [128, 0, 0], [378, 1134, 1176]] 92.1001
[[378, 902], [128, 0], [378, 1286], [378, 1030], [128, 0], [378, 390], [128, 0], [378, 2310]] 90.8373
[[756, 524, 0, 0], [128, 0, 0, 0], [756, 756, 152, 0], [756, 652, 0, 0], [128, 0, 0, 0], [756, 12, 0, 0], [128, 0, 0, 0], [756, 756, 756, 420]] 98.2977
[[756, 524, 0], [128, 0, 0], [756, 756, 152], [756, 652, 0], [128, 0, 0], [756, 12, 0], [128, 0, 0], [756, 756, 1176]] 99.9418
[[756, 524, 0], [128, 0, 0], [756, 908, 0], [756, 652, 0], [128, 0, 0], [756, 12, 0], [128, 0, 0], [756, 1134, 798]] 99.3333

[[756, 524], [128, 0], [756, 908], [756, 652], [128, 0], [756, 12], [128, 0], [756, 1932]] 98.1645

[[1134, 146], [128, 0], [1134, 530], [1134, 274], [128, 0], [768, 0], [128, 0], [1134, 1554]] 95.7867
[[1280, 0], [128, 0], [1512, 152], [1408, 0], [128, 0], [768, 0], [128, 0], [1512, 1176]] 90.6231
[[1280], [128], [1664], [1408], [128], [768], [128], [2688]] 96.3147
Best solution:  [[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0], [378, 378, 378, 274, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 420]]
Solution saved.
 62%|███████████████████████████████████                     | 5/8 [1:24:40<1:16:24, 1528.32s/it]Transfer matrix: 
tensor([[3584, 3072],
        [ 512, 1024]], dtype=torch.int32)
Start exhaustive searching.
[[252, 252, 252, 252, 252, 252, 152], [252, 132, 0, 0, 0, 0, 0]] 7.5448
[[252, 252, 252, 252, 252, 404], [252, 132, 0, 0, 0, 0]] 7.8546
[[252, 252, 252, 252, 656], [252, 132, 0, 0, 0]] 8.3330
[[252, 252, 252, 504, 404], [252, 132, 0, 0, 0]] 7.8569
[[252, 252, 252, 908], [252, 132, 0, 0]] 8.8965
[[252, 252, 504, 656], [252, 132, 0, 0]] 8.3379
[[252, 252, 1160], [252, 132, 0]] 9.3710
[[252, 504, 504, 404], [252, 132, 0, 0]] 7.8982
[[252, 504, 908], [252, 132, 0]] 8.8821
[[252, 756, 656], [252, 132, 0]] 8.3239

[[252, 1412], [252, 132]] 9.7244
[[504, 504, 656], [384, 0, 0]] 8.3366
[[504, 1160], [384, 0]] 9.3306
[[756, 908], [384, 0]] 8.8274
[[1664], [384]] 9.9176
Best solution:  [[252, 252, 252, 252, 252, 252, 152], [252, 132, 0, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[   0,    0,  512,  512],
        [3072, 1024, 1024, 1536],
        [1024, 3072, 2560, 1536],
        [   0,    0,    0,  512]], dtype=torch.int32)
Start exhaustive searching.
[[252, 4, 0, 0, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152, 0, 0], [252, 252, 252, 252, 252, 252, 252, 252, 32], [128, 0, 0, 0, 0, 0, 0, 0, 0]] 16.1071
[[252, 4, 0, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152, 0], [252, 252, 252, 252, 252, 252, 252, 284], [128, 0, 0, 0, 0, 0, 0, 0]] 16.2568
[[252, 4, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 252, 152], [252, 252, 252, 252, 252, 252, 536], [128, 0, 0, 0, 0, 0, 0]] 17.0696
[[252, 4, 0, 0, 0, 0, 0], [252, 252, 252, 252, 252, 404, 0], [252, 252, 252, 252, 252, 504, 284], [128, 0, 0, 0, 0, 0, 0]] 16.2489
[[252, 4, 0, 0, 0, 0], [252, 252, 252, 252, 252, 404], [252, 252, 252, 252, 252, 788], [128, 0, 0, 0, 0, 0]] 16.4207

[[252, 4, 0, 0, 0, 0], [252, 252, 252, 252, 504, 152], [252, 252, 252, 252, 504, 536], [128, 0, 0, 0, 0, 0]] 17.7816
[[252, 4, 0, 0, 0], [252, 252, 252, 252, 656], [252, 252, 252, 252, 1040], [128, 0, 0, 0, 0]] 16.6077
[[252, 4, 0, 0, 0, 0], [252, 252, 252, 504, 404, 0], [252, 252, 252, 504, 504, 284], [128, 0, 0, 0, 0, 0]] 16.1143
[[252, 4, 0, 0, 0], [252, 252, 252, 504, 404], [252, 252, 252, 504, 788], [128, 0, 0, 0, 0]] 16.6070
[[252, 4, 0, 0, 0], [252, 252, 252, 756, 152], [252, 252, 252, 756, 536], [128, 0, 0, 0, 0]] 17.8923
[[252, 4, 0, 0], [252, 252, 252, 908], [252, 252, 252, 1292], [128, 0, 0, 0]] 16.8743


[[252, 4, 0, 0, 0], [252, 252, 504, 504, 152], [252, 252, 504, 504, 536], [128, 0, 0, 0, 0]] 17.5318
[[252, 4, 0, 0], [252, 252, 504, 656], [252, 252, 504, 1040], [128, 0, 0, 0]] 16.8191
[[252, 4, 0, 0], [252, 252, 756, 404], [252, 252, 756, 788], [128, 0, 0, 0]] 17.4735
[[252, 4, 0], [252, 252, 1160], [252, 252, 1544], [128, 0, 0]] 18.1376
[[252, 4, 0, 0, 0], [252, 504, 504, 404, 0], [252, 504, 504, 504, 284], [128, 0, 0, 0, 0]] 15.8473
[[252, 4, 0, 0], [252, 504, 504, 404], [252, 504, 504, 788], [128, 0, 0, 0]] 17.0415
[[252, 4, 0, 0], [252, 504, 756, 152], [252, 504, 756, 536], [128, 0, 0, 0]] 17.6663
[[252, 4, 0], [252, 504, 908], [252, 504, 1292], [128, 0, 0]] 17.9995
[[252, 4, 0], [252, 756, 656], [252, 756, 1040], [128, 0, 0]] 16.8857
[[252, 4, 0], [252, 1008, 404], [252, 1008, 788], [128, 0, 0]] 17.9321
[[252, 4], [252, 1412], [252, 1796], [128, 0]] 18.7937
[[256, 0, 0, 0], [504, 504, 504, 152], [504, 504, 504, 536], [128, 0, 0, 0]] 18.2513
[[256, 0, 0], [504, 504, 656], [504, 504, 1040], [128, 0, 0]] 19.2370
[[256, 0, 0], [504, 756, 404], [504, 756, 788], [128, 0, 0]] 18.3328
[[256, 0], [504, 1160], [504, 1544], [128, 0]] 20.8753
[[256, 0, 0], [756, 756, 152], [756, 756, 536], [128, 0, 0]] 18.3294
[[256, 0], [756, 908], [756, 1292], [128, 0]] 18.9089
[[256, 0], [1008, 656], [1008, 1040], [128, 0]] 19.0647
[[256], [1664], [2048], [128]] 20.1419
Best solution:  [[252, 4, 0, 0, 0], [252, 504, 504, 404, 0], [252, 504, 504, 504, 284], [128, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[ 512,    0,    0, 1024,  512, 1024,  512, 1536],
        [   0,    0,    0,    0,  512,    0,    0,    0],
        [1024, 1536,    0,  512, 1536,  512,    0, 1536],
        [   0,  512,  512,  512,  512, 1024, 2048,  512],
        [   0,  512,    0,    0,    0,    0,    0,    0],
        [   0, 1024, 1024,  512,    0,    0,  512,    0],
        [   0,    0,  512,    0,    0,    0,    0,    0],
        [2560,  512, 2048, 1536, 1024, 1536, 1024,  512]], dtype=torch.int32)
Start exhaustive searching.
[[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0, 0], [378, 378, 378, 274, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 42]] 84.6976

[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0], [378, 378, 378, 274, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 420]] 85.8920
^[[B
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0], [378, 378, 378, 274, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 798]] 92.2195
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0], [378, 378, 378, 274, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 420]] 90.1801

[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 152], [378, 378, 378, 274, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1176]] 91.8982
[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 530, 0], [378, 378, 378, 274, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 798]] 91.7062
[[378, 378, 378, 146], [128, 0, 0, 0], [378, 378, 378, 530], [378, 378, 378, 274], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 378, 1554]] 90.5227
[[378, 378, 524, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 152, 0], [378, 378, 652, 0, 0], [128, 0, 0, 0, 0], [378, 378, 12, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 420]] 88.7956

[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 756, 152], [378, 378, 652, 0], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 756, 1176]] 93.6845
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 908, 0], [378, 378, 652, 0], [128, 0, 0, 0], [378, 378, 12, 0], [128, 0, 0, 0], [378, 378, 1134, 798]] 94.3412
[[378, 378, 524], [128, 0, 0], [378, 378, 908], [378, 378, 652], [128, 0, 0], [378, 378, 12], [128, 0, 0], [378, 378, 1932]] 93.7985
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 530, 0], [378, 756, 274, 0], [128, 0, 0, 0], [378, 390, 0, 0], [128, 0, 0, 0], [378, 756, 756, 798]] 94.7600
[[378, 756, 146], [128, 0, 0], [378, 756, 530], [378, 756, 274], [128, 0, 0], [378, 390, 0], [128, 0, 0], [378, 756, 1554]] 92.2630
[[378, 902, 0], [128, 0, 0], [378, 1134, 152], [378, 1030, 0], [128, 0, 0], [378, 390, 0], [128, 0, 0], [378, 1134, 1176]] 92.4893
[[378, 902], [128, 0], [378, 1286], [378, 1030], [128, 0], [378, 390], [128, 0], [378, 2310]] 91.6538
[[756, 524, 0, 0], [128, 0, 0, 0], [756, 756, 152, 0], [756, 652, 0, 0], [128, 0, 0, 0], [756, 12, 0, 0], [128, 0, 0, 0], [756, 756, 756, 420]] 92.1669
[[756, 524, 0], [128, 0, 0], [756, 756, 152], [756, 652, 0], [128, 0, 0], [756, 12, 0], [128, 0, 0], [756, 756, 1176]] 94.4981
[[756, 524, 0], [128, 0, 0], [756, 908, 0], [756, 652, 0], [128, 0, 0], [756, 12, 0], [128, 0, 0], [756, 1134, 798]] 99.9450
[[756, 524], [128, 0], [756, 908], [756, 652], [128, 0], [756, 12], [128, 0], [756, 1932]] 97.0780


[[1134, 146], [128, 0], [1134, 530], [1134, 274], [128, 0], [768, 0], [128, 0], [1134, 1554]] 94.8052
[[1280, 0], [128, 0], [1512, 152], [1408, 0], [128, 0], [768, 0], [128, 0], [1512, 1176]] 95.2787

[[1280], [128], [1664], [1408], [128], [768], [128], [2688]] 101.4790
Best solution:  [[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 152, 0, 0, 0], [378, 378, 378, 274, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 12, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 42]]
Solution saved.
 75%|██████████████████████████████████████████              | 6/8 [2:18:27<1:10:11, 2105.69s/it]Transfer matrix: 
tensor([[4096, 3072],
        [1024, 2048]], dtype=torch.int32)
Start exhaustive searching.
[[252, 252, 252, 252, 252, 252, 252, 28], [252, 252, 252, 12, 0, 0, 0, 0]] 4.3909
[[252, 252, 252, 252, 252, 252, 280], [252, 252, 252, 12, 0, 0, 0]] 4.4428
[[252, 252, 252, 252, 252, 532], [252, 252, 252, 12, 0, 0]] 4.7217
[[252, 252, 252, 252, 504, 280], [252, 252, 252, 12, 0, 0]] 4.4642
[[252, 252, 252, 252, 784], [252, 252, 252, 12, 0]] 5.1240
[[252, 252, 252, 504, 532], [252, 252, 252, 12, 0]] 4.7138
[[252, 252, 252, 1036], [252, 252, 252, 12]] 5.6648
[[252, 252, 504, 504, 280], [252, 252, 264, 0, 0]] 4.4017
[[252, 252, 504, 784], [252, 252, 264, 0]] 5.0982
[[252, 252, 756, 532], [252, 252, 264, 0]] 4.7020
[[252, 252, 1288], [252, 252, 264]] 5.9077
[[252, 504, 504, 532], [252, 504, 12, 0]] 4.7099
[[252, 504, 1036], [252, 504, 12]] 5.6450
[[252, 756, 784], [252, 516, 0]] 5.0930
[[252, 1540], [252, 516]] 6.8005
[[504, 504, 504, 280], [504, 264, 0, 0]] 4.4090
[[504, 504, 784], [504, 264, 0]] 5.0849
[[504, 756, 532], [504, 264, 0]] 4.6731
[[504, 1288], [504, 264]] 5.8761
[[756, 1036], [756, 12]] 5.6273
[[1008, 784], [768, 0]] 5.1584
[[1792], [768]] 6.9535
Best solution:  [[252, 252, 252, 252, 252, 252, 252, 28], [252, 252, 252, 12, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[   0,    0,  512,  512],
        [3072, 1536, 1536, 2048],
        [2048, 3584, 3072, 2048],
        [   0,    0,    0,  512]], dtype=torch.int32)
Start exhaustive searching.
[[256, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158, 0, 0], [378, 378, 378, 378, 378, 378, 378, 42], [128, 0, 0, 0, 0, 0, 0, 0]] 22.6516
[[256, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158, 0], [378, 378, 378, 378, 378, 378, 420], [128, 0, 0, 0, 0, 0, 0]] 23.1295
[[256, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158], [378, 378, 378, 378, 378, 798], [128, 0, 0, 0, 0, 0]] 22.6659
[[256, 0, 0, 0, 0, 0], [378, 378, 378, 378, 536, 0], [378, 378, 378, 378, 756, 420], [128, 0, 0, 0, 0, 0]] 22.5903
[[256, 0, 0, 0, 0], [378, 378, 378, 378, 536], [378, 378, 378, 378, 1176], [128, 0, 0, 0, 0]] 22.4053
[[256, 0, 0, 0, 0], [378, 378, 378, 756, 158], [378, 378, 378, 756, 798], [128, 0, 0, 0, 0]] 23.0122
[[256, 0, 0, 0], [378, 378, 378, 914], [378, 378, 378, 1554], [128, 0, 0, 0]] 23.4859
[[256, 0, 0, 0, 0], [378, 378, 756, 536, 0], [378, 378, 756, 756, 420], [128, 0, 0, 0, 0]] 22.9852
[[256, 0, 0, 0], [378, 378, 756, 536], [378, 378, 756, 1176], [128, 0, 0, 0]] 22.9295
[[256, 0, 0, 0], [378, 378, 1134, 158], [378, 378, 1134, 798], [128, 0, 0, 0]] 23.0635
[[256, 0, 0], [378, 378, 1292], [378, 378, 1932], [128, 0, 0]] 25.0528

[[256, 0, 0, 0], [378, 756, 756, 158], [378, 756, 756, 798], [128, 0, 0, 0]] 22.5217
[[256, 0, 0], [378, 756, 914], [378, 756, 1554], [128, 0, 0]] 23.0339
[[256, 0, 0], [378, 1134, 536], [378, 1134, 1176], [128, 0, 0]] 22.4446

[[256, 0], [378, 1670], [378, 2310], [128, 0]] 24.6029
[[256, 0, 0, 0], [756, 756, 536, 0], [756, 756, 756, 420], [128, 0, 0, 0]] 23.0912
[[256, 0, 0], [756, 756, 536], [756, 756, 1176], [128, 0, 0]] 22.8923
[[256, 0, 0], [756, 1134, 158], [756, 1134, 798], [128, 0, 0]] 23.0087
[[256, 0], [756, 1292], [756, 1932], [128, 0]] 24.2247
[[256, 0], [1134, 914], [1134, 1554], [128, 0]] 23.2171

[[256, 0], [1512, 536], [1512, 1176], [128, 0]] 23.5870
[[256], [2048], [2688], [128]] 26.0761
Best solution:  [[256, 0, 0, 0, 0], [378, 378, 378, 378, 536], [378, 378, 378, 378, 1176], [128, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[ 512,    0,    0, 1024,  512, 1024,  512, 1536],
        [   0,    0,    0,    0,  512,    0,    0,    0],
        [1536, 2048, 1024, 1024, 2048, 1024,    0, 1536],
        [ 512, 1024,  512,  512, 1024, 1536, 2048,  512],
        [   0,  512,    0,    0,    0,    0,    0,    0],
        [   0, 1024, 1024,  512,    0,    0, 1024,  512],
        [   0,    0,  512,    0,    0,    0,    0,    0],
        [2560,  512, 2048, 2048, 1024, 1536, 1536, 1024]], dtype=torch.int32)
Start exhaustive searching.
\\
[[378, 378, 378, 146, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0, 0], [378, 378, 378, 378, 378, 30, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 378, 48]] 91.0476
[[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0], [378, 378, 378, 378, 378, 30, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 426]] 93.6752
[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292], [378, 378, 378, 378, 378, 30, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 804]] 97.8785

[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 670, 0], [378, 378, 378, 378, 378, 30, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 756, 426]] 94.7866

[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 670], [378, 378, 378, 378, 378, 30], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 1182]] 96.1007
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 292], [378, 378, 378, 378, 408, 0], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 804]] 99.1022


[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1048], [378, 378, 378, 378, 408], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1560]] 99.8924
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 756, 670, 0], [378, 378, 378, 756, 30, 0], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 756, 756, 426]] 95.3571
[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 670], [378, 378, 378, 756, 30], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 1182]] 99.3677
[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 1134, 292], [378, 378, 378, 786, 0], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 1134, 804]] 99.6962
[[378, 378, 378, 146], [128, 0, 0, 0], [378, 378, 378, 1426], [378, 378, 378, 786], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 378, 1938]] 102.4430
[[378, 378, 524, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 292], [378, 378, 756, 408, 0], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 804]] 102.0495
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 756, 1048], [378, 378, 756, 408], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 756, 1560]] 101.4286
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 1134, 670], [378, 378, 1134, 30], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 1134, 1182]] 99.7335
[[378, 378, 524], [128, 0, 0], [378, 378, 1804], [378, 378, 1164], [128, 0, 0], [378, 378, 268], [128, 0, 0], [378, 378, 2316]] 97.5933
[[378, 756, 146, 0, 0], [128, 0, 0, 0, 0], [378, 756, 756, 670, 0], [378, 756, 756, 30, 0], [128, 0, 0, 0, 0], [378, 646, 0, 0, 0], [128, 0, 0, 0, 0], [378, 756, 756, 756, 426]] 97.1045
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 756, 670], [378, 756, 756, 30], [128, 0, 0, 0], [378, 646, 0, 0], [128, 0, 0, 0], [378, 756, 756, 1182]] 101.8570
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 1134, 292], [378, 756, 786, 0], [128, 0, 0, 0], [378, 646, 0, 0], [128, 0, 0, 0], [378, 756, 1134, 804]] 99.5613
[[378, 756, 146], [128, 0, 0], [378, 756, 1426], [378, 756, 786], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 756, 1938]] 100.9807

[[378, 902, 0], [128, 0, 0], [378, 1134, 1048], [378, 1134, 408], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 1134, 1560]] 102.4223
[[378, 902, 0], [128, 0, 0], [378, 1512, 670], [378, 1512, 30], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 1512, 1182]] 100.3412
[[378, 902], [128, 0], [378, 2182], [378, 1542], [128, 0], [378, 646], [128, 0], [378, 2694]] 92.0442
[[756, 524, 0, 0], [128, 0, 0, 0], [756, 756, 756, 292], [756, 756, 408, 0], [128, 0, 0, 0], [756, 268, 0, 0], [128, 0, 0, 0], [756, 756, 756, 804]] 110.8555
[[756, 524, 0], [128, 0, 0], [756, 756, 1048], [756, 756, 408], [128, 0, 0], [756, 268, 0], [128, 0, 0], [756, 756, 1560]] 109.6387
[[756, 524, 0], [128, 0, 0], [756, 1134, 670], [756, 1134, 30], [128, 0, 0], [756, 268, 0], [128, 0, 0], [756, 1134, 1182]] 102.9611
[[756, 524], [128, 0], [756, 1804], [756, 1164], [128, 0], [756, 268], [128, 0], [756, 2316]] 103.5844

[[1134, 146, 0], [128, 0, 0], [1134, 1134, 292], [1134, 786, 0], [128, 0, 0], [1024, 0, 0], [128, 0, 0], [1134, 1134, 804]] 104.3047
[[1134, 146], [128, 0], [1134, 1426], [1134, 786], [128, 0], [1024, 0], [128, 0], [1134, 1938]] 105.0575
[[1280, 0], [128, 0], [1512, 1048], [1512, 408], [128, 0], [1024, 0], [128, 0], [1512, 1560]] 105.2237

[[1280], [128], [2560], [1920], [128], [1024], [128], [3072]] 92.7961
Best solution:  [[378, 378, 378, 146, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0, 0], [378, 378, 378, 378, 378, 30, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 378, 48]]
Solution saved.
 88%|██████████████████████████████████████████████████▊       | 7/8 [3:29:39<46:53, 2813.90s/it]Transfer matrix: 
tensor([[4096, 3072],
        [1024, 2048]], dtype=torch.int32)
Start exhaustive searching.
[[252, 252, 252, 252, 252, 252, 252, 28], [252, 252, 252, 12, 0, 0, 0, 0]] 7.7907
[[252, 252, 252, 252, 252, 252, 280], [252, 252, 252, 12, 0, 0, 0]] 8.2998
[[252, 252, 252, 252, 252, 532], [252, 252, 252, 12, 0, 0]] 8.5490
[[252, 252, 252, 252, 504, 280], [252, 252, 252, 12, 0, 0]] 8.2445
[[252, 252, 252, 252, 784], [252, 252, 252, 12, 0]] 8.9814
[[252, 252, 252, 504, 532], [252, 252, 252, 12, 0]] 8.5696
[[252, 252, 252, 1036], [252, 252, 252, 12]] 9.5470
[[252, 252, 504, 504, 280], [252, 252, 264, 0, 0]] 8.2470
[[252, 252, 504, 784], [252, 252, 264, 0]] 8.9280
[[252, 252, 756, 532], [252, 252, 264, 0]] 8.5416
[[252, 252, 1288], [252, 252, 264]] 9.8723
[[252, 504, 504, 532], [252, 504, 12, 0]] 8.5562
[[252, 504, 1036], [252, 504, 12]] 9.5412
[[252, 756, 784], [252, 516, 0]] 8.8961
[[252, 1540], [252, 516]] 10.7216
[[504, 504, 504, 280], [504, 264, 0, 0]] 8.2225
[[504, 504, 784], [504, 264, 0]] 8.8828
[[504, 756, 532], [504, 264, 0]] 8.4826
[[504, 1288], [504, 264]] 9.8229
[[756, 1036], [756, 12]] 9.5212
[[1008, 784], [768, 0]] 8.8560
[[1792], [768]] 10.4778
Best solution:  [[252, 252, 252, 252, 252, 252, 252, 28], [252, 252, 252, 12, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[   0,    0,  512,  512],
        [3072, 1536, 1536, 2048],
        [2048, 3584, 3072, 2048],
        [   0,    0,    0,  512]], dtype=torch.int32)
Start exhaustive searching.
[[256, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158, 0, 0], [378, 378, 378, 378, 378, 378, 378, 42], [128, 0, 0, 0, 0, 0, 0, 0]] 23.0651
[[256, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158, 0], [378, 378, 378, 378, 378, 378, 420], [128, 0, 0, 0, 0, 0, 0]] 23.2732
[[256, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 158], [378, 378, 378, 378, 378, 798], [128, 0, 0, 0, 0, 0]] 23.3017
[[256, 0, 0, 0, 0, 0], [378, 378, 378, 378, 536, 0], [378, 378, 378, 378, 756, 420], [128, 0, 0, 0, 0, 0]] 22.3373
[[256, 0, 0, 0, 0], [378, 378, 378, 378, 536], [378, 378, 378, 378, 1176], [128, 0, 0, 0, 0]] 23.6020
[[256, 0, 0, 0, 0], [378, 378, 378, 756, 158], [378, 378, 378, 756, 798], [128, 0, 0, 0, 0]] 24.7040
[[256, 0, 0, 0], [378, 378, 378, 914], [378, 378, 378, 1554], [128, 0, 0, 0]] 25.2914
[[256, 0, 0, 0, 0], [378, 378, 756, 536, 0], [378, 378, 756, 756, 420], [128, 0, 0, 0, 0]] 23.8415
[[256, 0, 0, 0], [378, 378, 756, 536], [378, 378, 756, 1176], [128, 0, 0, 0]] 25.8582
[[256, 0, 0, 0], [378, 378, 1134, 158], [378, 378, 1134, 798], [128, 0, 0, 0]] 25.7834
[[256, 0, 0], [378, 378, 1292], [378, 378, 1932], [128, 0, 0]] 27.4251
[[256, 0, 0, 0], [378, 756, 756, 158], [378, 756, 756, 798], [128, 0, 0, 0]] 26.3356
[[256, 0, 0], [378, 756, 914], [378, 756, 1554], [128, 0, 0]] 27.5658
[[256, 0, 0], [378, 1134, 536], [378, 1134, 1176], [128, 0, 0]] 28.2831
[[256, 0], [378, 1670], [378, 2310], [128, 0]] 29.7896
[[256, 0, 0, 0], [756, 756, 536, 0], [756, 756, 756, 420], [128, 0, 0, 0]] 26.5687
[[256, 0, 0], [756, 756, 536], [756, 756, 1176], [128, 0, 0]] 28.7795
[[256, 0, 0], [756, 1134, 158], [756, 1134, 798], [128, 0, 0]] 26.6599
[[256, 0], [756, 1292], [756, 1932], [128, 0]] 30.3464
[[256, 0], [1134, 914], [1134, 1554], [128, 0]] 26.7599
[[256, 0], [1512, 536], [1512, 1176], [128, 0]] 27.2965
[[256], [2048], [2688], [128]] 33.4669
Best solution:  [[256, 0, 0, 0, 0, 0], [378, 378, 378, 378, 536, 0], [378, 378, 378, 378, 756, 420], [128, 0, 0, 0, 0, 0]]
Solution saved.
Transfer matrix: 
tensor([[ 512,    0,    0, 1024,  512, 1024,  512, 1536],
        [   0,    0,    0,    0,  512,    0,    0,    0],
        [1536, 2048, 1024, 1024, 2048, 1024,    0, 1536],
        [ 512, 1024,  512,  512, 1024, 1536, 2048,  512],
        [   0,  512,    0,    0,    0,    0,    0,    0],
        [   0, 1024, 1024,  512,    0,    0, 1024,  512],
        [   0,    0,  512,    0,    0,    0,    0,    0],
        [2560,  512, 2048, 2048, 1024, 1536, 1536, 1024]], dtype=torch.int32)
Start exhaustive searching.
[[378, 378, 378, 146, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0, 0], [378, 378, 378, 378, 378, 30, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 378, 48]] 93.8981
[[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0], [378, 378, 378, 378, 378, 30, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 426]] 93.5638
[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292], [378, 378, 378, 378, 378, 30, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 804]] 100.6201
[[378, 378, 378, 146, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 670, 0], [378, 378, 378, 378, 378, 30, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 756, 426]] 96.9988
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 670], [378, 378, 378, 378, 378, 30], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 1182]] 99.3527
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 292], [378, 378, 378, 378, 408, 0], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 378, 756, 804]] 103.2232
 [[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1048], [378, 378, 378, 378, 408], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 378, 1560]] 103.9150
[[378, 378, 378, 146, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 756, 670, 0], [378, 378, 378, 756, 30, 0], [128, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0], [128, 0, 0, 0, 0, 0], [378, 378, 378, 756, 756, 426]] 97.8931
[[[z[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 670], [378, 378, 378, 756, 30], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 756, 1182]] 99.8442
hn[[378, 378, 378, 146, 0], [128, 0, 0, 0, 0], [378, 378, 378, 1134, 292], [378, 378, 378, 786, 0], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 378, 1134, 804]] 102.2746
[[378, 378, 378, 146], [128, 0, 0, 0], [378, 378, 378, 1426], [378, 378, 378, 786], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 378, 1938]] 103.0240
[[378, 378, 524, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 292], [378, 378, 756, 408, 0], [128, 0, 0, 0, 0], [378, 378, 268, 0, 0], [128, 0, 0, 0, 0], [378, 378, 756, 756, 804]] 104.2440
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 756, 1048], [378, 378, 756, 408], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 756, 1560]] 103.1116
[[378, 378, 524, 0], [128, 0, 0, 0], [378, 378, 1134, 670], [378, 378, 1134, 30], [128, 0, 0, 0], [378, 378, 268, 0], [128, 0, 0, 0], [378, 378, 1134, 1182]] 102.7625
[[378, 378, 524], [128, 0, 0], [378, 378, 1804], [378, 378, 1164], [128, 0, 0], [378, 378, 268], [128, 0, 0], [378, 378, 2316]] 98.5602
[[378, 756, 146, 0, 0], [128, 0, 0, 0, 0], [378, 756, 756, 670, 0], [378, 756, 756, 30, 0], [128, 0, 0, 0, 0], [378, 646, 0, 0, 0], [128, 0, 0, 0, 0], [378, 756, 756, 756, 426]] 102.0148
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 756, 670], [378, 756, 756, 30], [128, 0, 0, 0], [378, 646, 0, 0], [128, 0, 0, 0], [378, 756, 756, 1182]] 101.7215
[[378, 756, 146, 0], [128, 0, 0, 0], [378, 756, 1134, 292], [378, 756, 786, 0], [128, 0, 0, 0], [378, 646, 0, 0], [128, 0, 0, 0], [378, 756, 1134, 804]] 105.6363
[[378, 756, 146], [128, 0, 0], [378, 756, 1426], [378, 756, 786], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 756, 1938]] 105.9685
[[378, 902, 0], [128, 0, 0], [378, 1134, 1048], [378, 1134, 408], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 1134, 1560]] 103.6491
[[378, 902, 0], [128, 0, 0], [378, 1512, 670], [378, 1512, 30], [128, 0, 0], [378, 646, 0], [128, 0, 0], [378, 1512, 1182]] 102.8223
[[378, 902], [128, 0], [378, 2182], [378, 1542], [128, 0], [378, 646], [128, 0], [378, 2694]] 96.4629
[[756, 524, 0, 0], [128, 0, 0, 0], [756, 756, 756, 292], [756, 756, 408, 0], [128, 0, 0, 0], [756, 268, 0, 0], [128, 0, 0, 0], [756, 756, 756, 804]] 113.9616
[[756, 524, 0], [128, 0, 0], [756, 756, 1048], [756, 756, 408], [128, 0, 0], [756, 268, 0], [128, 0, 0], [756, 756, 1560]] 110.7885
[[756, 524, 0], [128, 0, 0], [756, 1134, 670], [756, 1134, 30], [128, 0, 0], [756, 268, 0], [128, 0, 0], [756, 1134, 1182]] 106.6435
[[756, 524], [128, 0], [756, 1804], [756, 1164], [128, 0], [756, 268], [128, 0], [756, 2316]] 104.0924
[[1134, 146, 0], [128, 0, 0], [1134, 1134, 292], [1134, 786, 0], [128, 0, 0], [1024, 0, 0], [128, 0, 0], [1134, 1134, 804]] 110.3946
[[1134, 146], [128, 0], [1134, 1426], [1134, 786], [128, 0], [1024, 0], [128, 0], [1134, 1938]] 108.7038
[[1280, 0], [128, 0], [1512, 1048], [1512, 408], [128, 0], [1024, 0], [128, 0], [1512, 1560]] 108.2057
[[1280], [128], [2560], [1920], [128], [1024], [128], [3072]] 107.8058
Best solution:  [[378, 378, 378, 146, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 292, 0], [378, 378, 378, 378, 378, 30, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 268, 0, 0, 0, 0, 0], [128, 0, 0, 0, 0, 0, 0, 0], [378, 378, 378, 378, 378, 378, 378, 426]]
Solution saved.
100%|██████████████████████████████████████████████████████████| 8/8 [4:45:19<00:00, 2139.93s/it]
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# \\
bash: \: command not found
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune#  [[[zhn^C
bash: $'[[[zhn\003': command not found
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/tune# cd ../evaluation
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e1_correctness.py
Randomly select 10 cases for correctness testing...
Processing: M=512, N=8192, K=16384, comm_op=all_to_all, world_size=2
Processing: M=2048, N=8192, K=8192, comm_op=all_to_all, world_size=4
Processing: M=8192, N=8192, K=16384, comm_op=all_reduce, world_size=8
Successfully executed m8192n8192k16384_gefo_all_reduce_8.json
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True

Processing: M=1024, N=8192, K=16384, comm_op=all_to_all, world_size=2
Processing: M=3072, N=8192, K=8192, comm_op=reduce_scatter, world_size=2
Error executing m3072n8192k8192_gefo_reduce_scatter_2.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


Processing: M=2560, N=8192, K=8192, comm_op=all_to_all, world_size=8
Processing: M=2560, N=8192, K=16384, comm_op=all_to_all, world_size=2
Processing: M=3072, N=8192, K=8192, comm_op=reduce_scatter, world_size=8
Error executing m3072n8192k8192_gefo_reduce_scatter_8.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 5 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


Processing: M=8192, N=8192, K=16384, comm_op=reduce_scatter, world_size=2
Error executing m8192n8192k16384_gefo_reduce_scatter_2.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


Processing: M=2048, N=8192, K=16384, comm_op=reduce_scatter, world_size=8
Error executing m2048n8192k16384_gefo_reduce_scatter_8.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 5 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e1_correctness.py
Randomly select 10 cases for correctness testing...
Processing: M=8192, N=8192, K=8192, comm_op=all_reduce, world_size=2
Successfully executed m8192n8192k8192_gefo_all_reduce_2.json
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True

Processing: M=4096, N=8192, K=8192, comm_op=reduce_scatter, world_size=2
Error executing m4096n8192k8192_gefo_reduce_scatter_2.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


Processing: M=3072, N=8192, K=8192, comm_op=reduce_scatter, world_size=2
Error executing m3072n8192k8192_gefo_reduce_scatter_2.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


Processing: M=2048, N=8192, K=16384, comm_op=all_reduce, world_size=4
Successfully executed m2048n8192k16384_gefo_all_reduce_4.json
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True

Processing: M=2560, N=8192, K=16384, comm_op=all_to_all, world_size=8
Processing: M=1024, N=8192, K=16384, comm_op=all_to_all, world_size=4
Processing: M=4096, N=8192, K=16384, comm_op=all_reduce, world_size=8
Successfully executed m4096n8192k16384_gefo_all_reduce_8.json
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True

Processing: M=2048, N=8192, K=16384, comm_op=all_to_all, world_size=4
Processing: M=3072, N=8192, K=16384, comm_op=all_reduce, world_size=2
Successfully executed m3072n8192k16384_gefo_all_reduce_2.json
[GEMM+AllReduce+RMSNorm] all close :  True
[GEMM+AllReduce+RMSNorm] all close :  True

Processing: M=2048, N=8192, K=16384, comm_op=reduce_scatter, world_size=2
Error executing m2048n8192k16384_gefo_reduce_scatter_2.json:
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 108, in <module>
    main()
  File "/root/FlashOverlap/evaluation/../example/correctness_rs.py", line 101, in main
    mp.spawn(
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/root/FlashOverlap/example/correctness_rs.py", line 58, in per_gpu_process
    max_idx_unravel = torch.unravel_index(max_idx_flat, diff.shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'unravel_index'


(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e1_speedup.py
🚀 Running: python test.py 8192 8192 16384 all_reduce 2                                                 
🚀 Running: python test.py 2048 8192 8192 all_reduce 2                                                  
🚀 Running: python test.py 2048 8192 8192 reduce_scatter 2                                              
🚀 Running: python test.py 2048 8192 8192 all_reduce 4                                                  
Running tests:   4%|██▏                                                | 3/70 [00:45<15:53, 14.23s/test]Running tests:   4%|██▏                                                | 3/70 [00:48<18:00, 16.12s/test]
Traceback (most recent call last):
  File "/root/FlashOverlap/evaluation/e1_speedup.py", line 129, in <module>
    main()
  File "/root/FlashOverlap/evaluation/e1_speedup.py", line 59, in main
    subprocess.run(cmd, check=True, capture_output=True, text=True)
  File "/opt/conda/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1154, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 2021, in _communicate
    ready = selector.select(timeout)
  File "/opt/conda/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
 Running: python test.py 4096 8192 8192 reduce_scatter 2                                              
🚀 Running: python test.py 4096 8192 8192 all_reduce 4                                                  
🚀 Running: python test.py 4096 8192 8192 reduce_scatter 4                                              
🚀 Running: python test.py 4096 8192 8192 all_reduce 8                                                  
🚀 Running: python test.py 4096 8192 8192 reduce_scatter 8                                              
🚀 Running: python test.py 4096 8192 16384 all_reduce 2                                                 
🚀 Running: python test.py 4096 8192 16384 reduce_scatter 2                                             
🚀 Running: python test.py 4096 8192 16384 all_reduce 4                                                 
🚀 Running: python test.py 4096 8192 16384 reduce_scatter 4                                             
🚀 Running: python test.py 4096 8192 16384 all_reduce 8                                                 
🚀 Running: python test.py 4096 8192 16384 reduce_scatter 8                                             
🚀 Running: python test.py 8192 8192 8192 all_reduce 2                                                  
🚀 Running: python test.py 8192 8192 8192 reduce_scatter 2                                              
🚀 Running: python test.py 8192 8192 8192 all_reduce 4                                                  
🚀 Running: python test.py 8192 8192 8192 reduce_scatter 4                                              
🚀 Running: python test.py 8192 8192 8192 all_reduce 8                                                  
🚀 Running: python test.py 8192 8192 8192 reduce_scatter 8                                              
🚀 Running: python test.py 8192 8192 16384 reduce_scatter 2                                             
🚀 Running: python test.py 8192 8192 16384 all_reduce 4                                                 
🚀 Running: python test.py 8192 8192 16384 reduce_scatter 4                                             
🚀 Running: python test.py 8192 8192 16384 all_reduce 8                                                 
🚀 Running: python test.py 8192 8192 16384 reduce_scatter 8                                             
🚀 Running: python test_a2a.py 512 8192 8192 2                                                          
🚀 Running: python test_a2a.py 512 8192 8192 4                                                          
🚀 Running: python test_a2a.py 512 8192 16384 2                                                         
🚀 Running: python test_a2a.py 512 8192 16384 4                                                         
🚀 Running: python test_a2a.py 1024 8192 8192 2                                                         
🚀 Running: python test_a2a.py 1024 8192 8192 4                                                         
🚀 Running: python test_a2a.py 1024 8192 8192 8                                                         
🚀 Running: python test_a2a.py 1024 8192 16384 2                                                        
🚀 Running: python test_a2a.py 1024 8192 16384 4                                                        
🚀 Running: python test_a2a.py 1024 8192 16384 8                                                        
🚀 Running: python test_a2a.py 2048 8192 8192 2                                                         
🚀 Running: python test_a2a.py 2048 8192 8192 4                                                         
🚀 Running: python test_a2a.py 2048 8192 8192 8                                                         
🚀 Running: python test_a2a.py 2048 8192 16384 2                                                        
🚀 Running: python test_a2a.py 2048 8192 16384 4                                                        
🚀 Running: python test_a2a.py 2048 8192 16384 8                                                        
🚀 Running: python test_a2a.py 2560 8192 8192 2                                                         
🚀 Running: python test_a2a.py 2560 8192 8192 4                                                         
🚀 Running: python test_a2a.py 2560 8192 8192 8                                                         
🚀 Running: python test_a2a.py 2560 8192 16384 2                                                        
🚀 Running: python test_a2a.py 2560 8192 16384 4                                                        
🚀 Running: python test_a2a.py 2560 8192 16384 8                                                        
Running tests: 100%|██████████████████████████████████████████████████| 70/70 [34:29<00:00, 29.56s/test]
✅ All tests completed.
Speedup Statistics by Primitive and GPU Combination:
===============================================================================================
Primitive       GPU        Baseline             Decomposition        FlashOverlap        
-----------------------------------------------------------------------------------------------
all_reduce      2          1.00(1.00-1.00)      1.20(1.16-1.25)      1.31(1.18-1.54)     
all_reduce      4          1.00(1.00-1.00)      1.17(1.11-1.23)      1.22(1.13-1.40)     
all_reduce      8          1.00(1.00-1.00)      1.13(1.07-1.18)      1.12(1.05-1.26)     
all_to_all      2          1.00(1.00-1.00)      1.24(1.14-1.38)      1.35(1.01-1.70)     
all_to_all      4          1.00(1.00-1.00)      1.11(1.06-1.22)      1.26(1.15-1.50)     
all_to_all      8          1.00(1.00-1.00)      0.98(0.91-1.05)      1.17(1.06-1.25)     
reduce_scatter  2          1.00(1.00-1.00)      1.19(1.11-1.24)      1.32(1.24-1.40)     
reduce_scatter  4          1.00(1.00-1.00)      1.19(1.15-1.24)      1.30(1.13-1.50)     
reduce_scatter  8          1.00(1.00-1.00)      1.19(1.11-1.30)      1.15(1.06-1.34)     
===============================================================================================
Note: Speedup values shown as mean(min-max), where baseline = 1.00
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation#   
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 

(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e3_rmsnorm_overhead.py^C
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e2_predictive_search.py
Processing config files:   0%|                                           | 0/5 [00:00<?, ?file/s]
Processing config files:  60%|████████████████████▍             | 3/5 [46:25<26:33, 796.53s/fi
Processing config files: 100%|████████████████████████████████| 5/5 [1:15:09<00:00, 901.95s/file]
Error: 9.0158 ± 4.7713
Relative Perf of Predictive Search: 99.02% ± 1.13%
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 

Relative Perf of Predictive Search: 99.02% ± 1.13%
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# 
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e2_predictive_search.py^C
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap/evaluation# python3 e3_gemm_overhead.py
  0%|                                                                     | 0/20 [00:00<?, ?it/s]Running warmup for the first configuration...
Warmup completed. Now testing the first configuration again for statistics...
100%|████████████████████████████████████████████████████████████| 20/20 [05:49<00:00, 17.49s/it]

=== Summary ===
Number of configurations tested: 20
Tile reorder overhead: 0.03% ± 0.22%
Token reorder overhead: 0.35% ± 0.55%
(base) root@oc-dbsdm6osd4yzh47x-devmachine-0:~/FlashOverlap